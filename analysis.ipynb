{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Analysis Notebook\n",
                "\n",
                "This notebook is designed to handle and analyze the large datasets found in this directory (`data1.csv` and `data2.csv`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "\n",
                "# Set plot style\n",
                "sns.set(style=\"whitegrid\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Define File Paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_DIR = \".\"\n",
                "FILE_1 = os.path.join(DATA_DIR, \"data1.csv\", \"data1.csv\") # Assuming the file is inside the directory of the same name based on previous exploration\n",
                "FILE_2 = os.path.join(DATA_DIR, \"data2.csv\", \"data2.csv\")\n",
                "\n",
                "print(f\"Checking files:\\n{FILE_1}: {os.path.exists(FILE_1)}\\n{FILE_2}: {os.path.exists(FILE_2)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Data Sample\n",
                "Since the files are very large (~10GB), we will first load a small sample to inspect the structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_sample(file_path, n_rows=1000):\n",
                "    try:\n",
                "        return pd.read_csv(file_path, nrows=n_rows)\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading {file_path}: {e}\")\n",
                "        return None\n",
                "\n",
                "df1_sample = load_sample(FILE_1)\n",
                "df2_sample = load_sample(FILE_2)\n",
                "\n",
                "if df1_sample is not None:\n",
                "    print(\"Data 1 Sample:\")\n",
                "    display(df1_sample.head())\n",
                "    print(df1_sample.info())\n",
                "\n",
                "if df2_sample is not None:\n",
                "    print(\"\\nData 2 Sample:\")\n",
                "    display(df2_sample.head())\n",
                "    print(df2_sample.info())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Process Data in Chunks\n",
                "To analyze the full dataset, we can iterate through it in chunks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "chunk_size = 100000\n",
                "\n",
                "def process_in_chunks(file_path, chunk_size=100000):\n",
                "    print(f\"Processing {file_path} in chunks of {chunk_size}...\")\n",
                "    # Example: Count total rows or compute a simple statistic\n",
                "    total_rows = 0\n",
                "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
                "        # Perform your analysis here on 'chunk'\n",
                "        total_rows += len(chunk)\n",
                "        # break # Remove this break to process the whole file\n",
                "    print(f\"Total rows processed: {total_rows}\")\n",
                "\n",
                "# Uncomment to run (this might take a while for 10GB files)\n",
                "# process_in_chunks(FILE_1)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Analyze Time Interval\n",
                "The `TimeInterval` column contains Unix timestamps in milliseconds. Let's convert them to readable dates and verify the interval size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert TimeInterval to datetime\n",
                "if df1_sample is not None:\n",
                "    df1_sample['datetime'] = pd.to_datetime(df1_sample['TimeInterval'], unit='ms')\n",
                "    print(\"Converted Datetimes (First 5):\")\n",
                "    display(df1_sample[['TimeInterval', 'datetime']].head())\n",
                "    \n",
                "    # Calculate interval difference\n",
                "    unique_times = sorted(df1_sample['TimeInterval'].unique())\n",
                "    if len(unique_times) > 1:\n",
                "        diff = unique_times[1] - unique_times[0]\n",
                "        print(f\"\\nTime difference between steps: {diff} ms ({diff/1000/60} minutes)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}